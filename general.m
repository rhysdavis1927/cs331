(* ::Package:: *)

trainSize=64;
hiddenNeurons=8;
iterations=1000;
(*Logistic function:*)
f[x_]=1/(1+Exp[-x]); (*works on vectors component-wise*)

(*For the XOR problem with n hidden neurons*)
(*Defining weight matrices in-hidden:*)
intoh[n_]:=Table[4.Random[]-2,{n},{3}]
(*and hidden-op:*)
htout[n_]:=Table[4.Random[]-2,{n+1}] (*extra one for offset*)
(*For the 2-hidden unit network:*)
(*First set up weight matrices:*)
weight1=intoh[hiddenNeurons]; (*for 2 hidden units*)
weight2=htout[hiddenNeurons];
(*Then a table for the errors:*)
etab=Table[0,{1000}];
sampleetab=Table[0,{100}];

ones=Table[-1,{64}];
train16 = {{-0.28552913795916557`,0.7195684647840876`,0.6428144957304299`,-1.2214079369733455`,0.12156358903633763`,-0.02681021984035775`,0.5253360707042003`,0.023017785419832435`,1.3647068066067805`,0.6169169398171841`,-0.05097468069223843`,0.9621930482182198`,2.0039712581490337`,1.494941053721349`,1.8232834908098776`,1.3095628634378593`},{-0.1351850177267172`,-0.6461033522630971`,0.3936361253161326`,-0.4818122191520221`,0.8531248252818876`,0.4283287710031124`,0.9239675446441595`,2.3408069389964883`,0.12617277126971443`,-0.7840018877126417`,0.28585540924776964`,0.20172296638013348`,0.9227672145623982`,1.9034208974172464`,1.0814599466665706`,0.7678062251139949`},{-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1}};
train16out={0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0};

train32 = {{-0.28552913795916557`,0.7195684647840876`,0.6428144957304299`,-1.2214079369733455`,0.12156358903633763`,-0.02681021984035775`,0.5253360707042003`,0.023017785419832435`,1.3647068066067805`,0.6169169398171841`,-0.05097468069223843`,0.9621930482182198`,2.0039712581490337`,1.494941053721349`,1.8232834908098776`,1.3095628634378593`,-1.0147966566702842`,-0.9981210089769575`,-0.24653774590506272`,0.21248328797595567`,0.9820948173375194`,-0.2889487200360429`,0.15223140224400994`,-0.061342860161801786`,0.8180663203622411`,1.4070169662538414`,1.396608609796169`,0.19048752447837047`,1.9050498379614096`,0.6929348834662405`,1.6402384757155777`,1.5483652008732354`},{-0.1351850177267172`,-0.6461033522630971`,0.3936361253161326`,-0.4818122191520221`,0.8531248252818876`,0.4283287710031124`,0.9239675446441595`,2.3408069389964883`,0.12617277126971443`,-0.7840018877126417`,0.28585540924776964`,0.20172296638013348`,0.9227672145623982`,1.9034208974172464`,1.0814599466665706`,0.7678062251139949`,-0.06267965699616611`,-0.09738616455723009`,1.258834235172041`,-0.9181325633482432`,-0.25687284722426185`,0.8644818815061412`,0.6687933835399065`,1.952064396818376`,-0.03615542602466572`,-0.4558139510327045`,0.19052069816031544`,0.5088150890738556`,0.6971404022059422`,1.2709515185765843`,0.9113841549840859`,1.0686017159528414`},{-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1}};
train32out={0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0};

train64 = {{-0.28552913795916557`,0.7195684647840876`,0.6428144957304299`,-1.2214079369733455`,0.12156358903633763`,-0.02681021984035775`,0.5253360707042003`,0.023017785419832435`,1.3647068066067805`,0.6169169398171841`,-0.05097468069223843`,0.9621930482182198`,2.0039712581490337`,1.494941053721349`,1.8232834908098776`,1.3095628634378593`,-1.0147966566702842`,-0.9981210089769575`,-0.24653774590506272`,0.21248328797595567`,0.9820948173375194`,-0.2889487200360429`,0.15223140224400994`,-0.061342860161801786`,0.8180663203622411`,1.4070169662538414`,1.396608609796169`,0.19048752447837047`,1.9050498379614096`,0.6929348834662405`,1.6402384757155777`,1.5483652008732354`,0.016340791971944945`,-0.23038001209557799`,-0.7107890009361955`,-0.027572666641897163`,-0.6423710290392752`,0.20721514656866355`,0.4638334397460962`,-0.220954634491436`,1.7640567851210738`,1.1304867380274821`,1.5856568107056253`,0.9607934674418988`,1.915571645925219`,0.28430820376016785`,1.4457291593693227`,1.1443501738220583`,0.2511184458361606`,-0.27782042480327773`,-0.26514762701036043`,1.573784159705993`,-1.1806027101990317`,-0.15093383919266107`,0.024329712666091387`,-0.2767635635079185`,0.9887470193193172`,0.6064629465484408`,0.9874212871073181`,1.221826765609725`,0.0647986024120476`,1.6662926579399495`,0.8807584468279209`,1.403669949526489`},{-0.1351850177267172`,-0.6461033522630971`,0.3936361253161326`,-0.4818122191520221`,0.8531248252818876`,0.4283287710031124`,0.9239675446441595`,2.3408069389964883`,0.12617277126971443`,-0.7840018877126417`,0.28585540924776964`,0.20172296638013348`,0.9227672145623982`,1.9034208974172464`,1.0814599466665706`,0.7678062251139949`,-0.06267965699616611`,-0.09738616455723009`,1.258834235172041`,-0.9181325633482432`,-0.25687284722426185`,0.8644818815061412`,0.6687933835399065`,1.952064396818376`,-0.03615542602466572`,-0.4558139510327045`,0.19052069816031544`,0.5088150890738556`,0.6971404022059422`,1.2709515185765843`,0.9113841549840859`,1.0686017159528414`,0.497115966810609`,-0.08566041354091025`,-0.1312190846462577`,0.214168481168517`,1.6910609105626695`,0.18254147992330738`,0.24355000704707364`,1.6070925692284859`,-0.1921484529886443`,-0.04273725772783511`,0.6183753801293709`,-0.03890169232807942`,1.5500315697105913`,2.4064522561730706`,0.17044279917053184`,0.2862287925643351`,0.1228453314976391`,-0.06699458775642284`,0.24764565498789112`,-0.10379271596351411`,1.0314119597514295`,0.9764539827117836`,1.1060827967205518`,1.1185608821843231`,0.3570570407794814`,-0.2845433524112181`,-0.38797203539799296`,0.6032577251041649`,1.929916503654697`,0.49752586441982316`,1.673014074017238`,0.9442769439586642`},{-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1}};
(*train64out=Abs[{1,-1}.train64];*)
(*train64wnoise=Join[train64+Transpose[Take[noise1,64]],{Take[ones,64]}];*)
train64out= {0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0};


(*Target output*)
If[trainSize==16,trainop=train16out;train=train16wnoise;,0;]
If[trainSize==32,trainop=train32out;train=train32wnoise;,0;]
If[trainSize==64,trainop=train64out;train=train64wnoise;,0;]
(*{0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0};*)

(*Full set of training vectors*)

(*Training patterns are 3*16 matrix; last row is offset:*)
(*{{-0.310691, -0.309003, 1.25774, 1.31959, -0.0897083, -0.457115,
1.42524, 1.43962, -0.21377, -0.16744, 0.579612, 1.90558, 0.442017,0 .204012, 1.75664, 0.584128},
{0.0164278, 0.898471, -0.231735, 0.82952, -1.02045, 1.84369, 0.111823,
0.28365, 0.0759174, 0.985518, 0.584378, 0.434351, 0.35245, -0.0194183,
-0.336488, 1.45608}, {-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
-1, -1, -1, -1}};*)
(*Then the training loop of 1000 epochs:*)
Do[
hid=f[weight1.train]; (*hidden outputs*)
out=f[weight2.Join[hid,{Take[ones,trainSize]}]]; (*net output*)
e=trainop-out; (*output error*)
etab[[i]]=e.e/16; (*squared error on epoch i*)
deltaout=e*out(1-out); (*delta for output unit*)
ehid=Take[Outer[Times,weight2,deltaout],hiddenNeurons]; (*backpropagate,strip offset*)
deltahid=ehid*hid*(1-hid); (*get delta for hidden layer*)
weight2+=deltaout.Transpose[Join[hid,{Take[ones,trainSize]}]]; (*update output weight matrix*)
weight1+=deltahid.Transpose[train]; (*hidden weight matrix*)
If[Mod[i,10]==0,sampleetab[[i/10]]=etab[[i]],0]
(*If[Mod[i,100]==0,Print[etab16[[i]]],0]*),{i,iterations}]
hid=f[weight1.testset]; (*hidden outputs*)
out=f[weight2.Join[hid,{Take[ones,trainSize]}]]; (*net output*)
error=testout-out
error.error/trainSize
DensityPlot[f[weight2.Join[f[weight1.{{x},{y},{-1}}],{{-1}}]],{x,0,1},{y,0,1}]
ListLinePlot[etab]
ListLinePlot[sampleetab]



